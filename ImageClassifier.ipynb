{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150, 150\n",
    "\n",
    "#defining the data directories\n",
    "train_data_dir= 'data/train'\n",
    "validation_data_dir= 'data/validation'\n",
    "n_training_sample= 400\n",
    "n_validation_sample= 100\n",
    "epochs=20\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(2, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer = tf.optimizers.Adam(),\n",
    "#              loss = 'sparse_categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 40 steps, validate for 10 steps\n",
      "Epoch 1/20\n",
      "40/40 [==============================] - 17s 430ms/step - loss: 7.8280 - accuracy: 0.9775 - val_loss: 8.3814 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 16s 399ms/step - loss: 8.3411 - accuracy: 1.0000 - val_loss: 8.3814 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 3.8997 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 16s 401ms/step - loss: 0.6689 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 16s 388ms/step - loss: 0.6897 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 16s 388ms/step - loss: 0.7105 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 15s 385ms/step - loss: 0.6862 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 15s 384ms/step - loss: 0.7035 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 16s 401ms/step - loss: 0.7035 - accuracy: 0.9975 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 0.7070 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 16s 389ms/step - loss: 0.6897 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 16s 395ms/step - loss: 0.6862 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 15s 383ms/step - loss: 0.7243 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 16s 399ms/step - loss: 0.6931 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 16s 394ms/step - loss: 0.6689 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 16s 394ms/step - loss: 0.7035 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 16s 397ms/step - loss: 0.6724 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 17s 416ms/step - loss: 0.7174 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 17s 417ms/step - loss: 0.6793 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 14s 362ms/step - loss: 0.7313 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c63fea2748>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=n_training_sample // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=n_validation_sample // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "pred= image.load_img('data/test/2.jpg', target_size=(150,150))\n",
    "pred=image.img_to_array(pred)\n",
    "pred= np.expand_dims(pred, axis=0)\n",
    "result= model.predict(pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fruit in the image is Dog\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    answer='Dog'\n",
    "else:\n",
    "    answer='Cat'\n",
    "print(\"The fruit in the image is\",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
