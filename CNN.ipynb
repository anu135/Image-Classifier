{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 28,28\n",
    "\n",
    "#defining the data directories\n",
    "train_data_dir= 'data/train'\n",
    "validation_data_dir= 'data/validation'\n",
    "n_training_sample= 400\n",
    "n_validation_sample= 100\n",
    "epochs=20\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 3)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(2, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer = tf.optimizers.Adam(),\n",
    "#              loss = 'sparse_categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 40 steps, validate for 10 steps\n",
      "Epoch 1/20\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 0.7353 - accuracy: 0.4175 - val_loss: 0.6931 - val_accuracy: 0.9600\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 5s 128ms/step - loss: 0.6828 - accuracy: 0.4975 - val_loss: 0.6931 - val_accuracy: 0.3900\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 5s 133ms/step - loss: 0.6862 - accuracy: 0.3975 - val_loss: 0.6931 - val_accuracy: 0.9300\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 6s 146ms/step - loss: 0.6793 - accuracy: 0.7700 - val_loss: 0.6931 - val_accuracy: 0.3300\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 6s 138ms/step - loss: 0.7174 - accuracy: 0.6525 - val_loss: 0.6931 - val_accuracy: 0.9300\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 6s 146ms/step - loss: 0.7278 - accuracy: 0.6425 - val_loss: 0.6931 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 6s 140ms/step - loss: 0.6654 - accuracy: 0.9025 - val_loss: 0.6931 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 6s 152ms/step - loss: 0.6654 - accuracy: 0.7300 - val_loss: 0.6931 - val_accuracy: 0.1600\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 5s 132ms/step - loss: 0.7070 - accuracy: 0.4275 - val_loss: 0.6931 - val_accuracy: 0.0800\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 5s 129ms/step - loss: 0.6966 - accuracy: 0.0725 - val_loss: 0.6931 - val_accuracy: 0.1500\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 5s 133ms/step - loss: 0.6793 - accuracy: 0.6175 - val_loss: 0.6931 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 5s 129ms/step - loss: 0.6931 - accuracy: 0.9875 - val_loss: 0.6931 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 5s 128ms/step - loss: 0.6862 - accuracy: 0.9825 - val_loss: 0.6931 - val_accuracy: 0.9800\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 5s 119ms/step - loss: 0.7070 - accuracy: 0.8000 - val_loss: 0.6931 - val_accuracy: 0.3900\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 6s 146ms/step - loss: 0.7105 - accuracy: 0.5825 - val_loss: 0.6931 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 6s 150ms/step - loss: 0.6793 - accuracy: 1.0000 - val_loss: 0.6931 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 7s 163ms/step - loss: 0.6689 - accuracy: 0.9975 - val_loss: 0.6931 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 6s 144ms/step - loss: 0.7174 - accuracy: 0.9900 - val_loss: 0.6931 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 6s 144ms/step - loss: 0.6966 - accuracy: 0.9975 - val_loss: 0.6931 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 6s 148ms/step - loss: 0.6689 - accuracy: 0.9975 - val_loss: 0.6931 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2870f2b4c88>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=n_training_sample // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=n_validation_sample // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#testing the model\n",
    "pred= image.load_img('data/test/4.jpg', target_size=(28,28))\n",
    "pred=image.img_to_array(pred)\n",
    "pred= np.expand_dims(pred, axis=0)\n",
    "result= model.predict(pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fruit in the image is Dog\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    answer='Dog'\n",
    "else:\n",
    "    answer='Cat'\n",
    "print(\"The fruit in the image is\",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
